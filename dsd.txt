import psycopg2
import logging
import boto3
import csv
import io

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('migration.log')]
)

# AWS S3 credentials and bucket name
AWS_BUCKET_NAME = 'itx-bhq-orbit-datastore-dev'
AWS_REGION = 'us-east-1'
AWS_ACCESS_KEY = '#'
AWS_SECRET_KEY = '#'

# Redshift connection details
REDSHIFT_HOST = '#'
REDSHIFT_PORT = '5439'  # Default Redshift port
REDSHIFT_USER = '#'
REDSHIFT_PASSWORD = '#'
REDSHIFT_DB = 'orbit'

# S3 client setup
s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY, region_name=AWS_REGION)

# Function to connect to Redshift
def connect_redshift():
    try:
        conn = psycopg2.connect(
            dbname=REDSHIFT_DB,
            user=REDSHIFT_USER,
            password=REDSHIFT_PASSWORD,
            host=REDSHIFT_HOST,
            port=REDSHIFT_PORT
        )
        logging.info("Successfully connected to Redshift.")
        return conn
    except Exception as e:
        logging.error(f"Error connecting to Redshift: {e}")
        raise

# Function to get table names (only tables, not views)
def get_table_names():
    try:
        conn = connect_redshift()
        cursor = conn.cursor()
        cursor.execute(""" 
            SELECT table_name 
            FROM information_schema.tables
            WHERE table_schema = 'orbit_64407564mmy3009' 
              AND table_type = 'BASE TABLE';
        """)
        tables = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        logging.info(f"Retrieved table names: {tables}")
        return tables
    except Exception as e:
        logging.error(f"Error fetching table names: {e}")
        raise

# Function to migrate table data to S3
def migrate_table_data_to_s3(table_name):
    try:
        conn = connect_redshift()
        cursor = conn.cursor()
        cursor.execute(f"SELECT * FROM orbit_64407564mmy3009.{table_name};")
        rows = cursor.fetchall()
        
        # Convert data to CSV format in memory
        csv_buffer = io.StringIO()
        csv_writer = csv.writer(csv_buffer)
        
        # Write the header
        cursor.execute(f"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}'")
        columns = [row[0] for row in cursor.fetchall()]
        csv_writer.writerow(columns)
        
        # Write the data
        csv_writer.writerows(rows)
        
        # Upload CSV data to S3
        s3_key = f"snowflake_poc/{table_name}.csv"
        s3_client.put_object(Bucket=AWS_BUCKET_NAME, Key=s3_key, Body=csv_buffer.getvalue())
        logging.info(f"Uploaded data for table {table_name} to S3.")
        
        cursor.close()
        conn.close()
    except Exception as e:
        logging.error(f"Error migrating data for table {table_name}: {e}")
        raise

# Function to migrate table data to S3 (for all tables)
def migrate_data_to_s3():
    try:
        # Migrate tables and their data
        tables = get_table_names()
        for table in tables:
            logging.info(f"Starting export for table data: {table}")
            
            # Migrate table data
            migrate_table_data_to_s3(table)
            logging.info(f"Successfully exported data for table: {table}")
    
    except Exception as e:
        logging.error(f"Migration failed: {e}")
        raise

# Execute the migration
if __name__ == "__main__":
    logging.info("Migration process started.")
    try:
        migrate_data_to_s3()
        logging.info("All table data migrated successfully.")
    except Exception as e:
        logging.error(f"Migration process failed: {e}")
