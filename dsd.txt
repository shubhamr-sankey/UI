import pandas as pd
import psycopg2
import logging
import boto3
from io import StringIO

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('migration.log')]
)

# AWS S3 credentials and bucket name
AWS_BUCKET_NAME = 'itx-bhq-orbit-datastore-dev'
AWS_REGION = 'us-east-1'
AWS_ACCESS_KEY = 'AKIAU4A3W7LKBVA4Z5O4'
AWS_SECRET_KEY = 'your-secret-key'

# Redshift connection details
REDSHIFT_HOST = 'medai-orbit.cqibeghf1hd5.us-east-1.redshift.amazonaws.com'
REDSHIFT_PORT = '5439'  # Default Redshift port
REDSHIFT_USER = 'orbit_sa_read_only'
REDSHIFT_PASSWORD = '419iKpjj-zPX'
REDSHIFT_DB = 'orbit'

# S3 client setup
s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY, region_name=AWS_REGION)

# Function to connect to Redshift
def connect_redshift():
    try:
        conn = psycopg2.connect(
            dbname=REDSHIFT_DB,
            user=REDSHIFT_USER,
            password=REDSHIFT_PASSWORD,
            host=REDSHIFT_HOST,
            port=REDSHIFT_PORT
        )
        logging.info("Successfully connected to Redshift.")
        return conn
    except Exception as e:
        logging.error(f"Error connecting to Redshift: {e}")
        raise

# Function to get table names
def get_table_names():
    try:
        conn = connect_redshift()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.tables
            WHERE table_schema = 'orbit_64407564mmy3009' 
              AND table_type = 'BASE TABLE';
        """)
        tables = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        logging.info(f"Retrieved table names: {tables}")
        return tables
    except Exception as e:
        logging.error(f"Error fetching table names: {e}")
        raise

# Function to export table to CSV and upload to S3 in chunks
def upload_table_to_s3(table_name):
    try:
        conn = connect_redshift()
        cursor = conn.cursor()

        # Define chunk size
        chunk_size = 10000  # You can adjust this based on your memory constraints
        offset = 0

        # Fetch and upload data in chunks
        while True:
            query = f"SELECT * FROM orbit_64407564mmy3009.{table_name} LIMIT {chunk_size} OFFSET {offset}"
            cursor.execute(query)
            columns = [desc[0] for desc in cursor.description]
            rows = cursor.fetchall()

            if not rows:
                break

            # Convert data to a pandas DataFrame
            df = pd.DataFrame(rows, columns=columns)

            # Create a CSV buffer
            csv_buffer = StringIO()
            df.to_csv(csv_buffer, index=False)

            # Upload CSV to S3
            s3_client.put_object(Bucket=AWS_BUCKET_NAME, Key=f"redshift_tables/{table_name}_part_{offset//chunk_size + 1}.csv", Body=csv_buffer.getvalue())
            logging.info(f"Uploaded chunk {offset//chunk_size + 1} of table {table_name} to S3.")

            # Update offset for next chunk
            offset += chunk_size

        cursor.close()
        conn.close()

    except Exception as e:
        logging.error(f"Error exporting table {table_name}: {e}")
        raise

# Function to migrate all tables to S3
def migrate_tables_to_s3():
    try:
        tables = get_table_names()
        for table in tables:
            logging.info(f"Starting export for table: {table}")
            upload_table_to_s3(table)
            logging.info(f"Successfully exported table: {table}")
    except Exception as e:
        logging.error(f"Migration failed: {e}")
        raise

# Execute the migration
if __name__ == "__main__":
    logging.info("Migration process started.")
    try:
        migrate_tables_to_s3()
        logging.info("All tables migrated successfully.")
    except Exception as e:
        logging.error(f"Migration process failed: {e}")
