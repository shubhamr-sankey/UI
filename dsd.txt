import redshift_connector
import pandas as pd
import boto3
import io
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

# AWS S3 bucket details
S3_BUCKET = '#'
S3_PATH = '#'  # The location in the bucket (without filename)

REDSHIFT_HOST = '#'
REDSHIFT_PORT = '5439'  # Default Redshift port
REDSHIFT_USER = 'orbit_sa_read_only'
REDSHIFT_PASSWORD = '#'
REDSHIFT_DBNAME = 'orbit'
REDSHIFT_SCHEMA = '#'  # Schema containing the views

# Create Redshift connection
def get_redshift_connection():
    try:
        logger.info("Attempting to connect to Redshift...")
        conn = redshift_connector.connect(
            host=REDSHIFT_HOST,
            port=REDSHIFT_PORT,
            database=REDSHIFT_DBNAME,
            user=REDSHIFT_USER,
            password=REDSHIFT_PASSWORD
        )
        logger.info("Successfully connected to Redshift.")
        return conn
    except Exception as e:
        logger.error(f"Failed to connect to Redshift: {e}")
        raise

# Get a list of all views in the specified schema
def get_views_in_schema(conn, schema):
    try:
        logger.info(f"Fetching all views in schema: {schema}")
        query = f"""
        SELECT table_name 
        FROM information_schema.views 
        WHERE table_schema = '{schema}';
        """
        views_df = pd.read_sql(query, conn)
        views = views_df['table_name'].tolist()
        logger.info(f"Found {len(views)} views in schema '{schema}'.")
        return views
    except Exception as e:
        logger.error(f"Error fetching views in schema {schema}: {e}")
        raise

# Fetch the DDL for the Redshift view
def fetch_view_ddl(conn, view_name, schema_name):
    try:
        logger.info(f"Fetching DDL for view: {view_name} in schema: {schema_name}")
        
        # Query the pg_views system catalog to get the DDL of the view
        query = f"""
        SELECT 'CREATE VIEW ' || table_schema || '.' || table_name || ' AS ' || view_definition AS view_ddl
        FROM information_schema.views
        WHERE table_schema = '{schema_name}' AND table_name = '{view_name}';
        """
        
        # Fetch the DDL
        ddl_df = pd.read_sql(query, conn)
        
        if ddl_df.empty:
            logger.warning(f"No DDL found for view: {view_name}")
            return None
        
        view_ddl = ddl_df['view_ddl'].iloc[0]
        logger.info(f"Successfully fetched DDL for view: {view_name}.")
        return view_ddl
    except Exception as e:
        logger.error(f"Error while fetching DDL for view {view_name}: {e}")
        raise

# Upload the DDL to S3
def upload_to_s3(ddl, view_name):
    try:
        logger.info(f"Converting DDL to CSV format for view: {view_name}.")
        # Convert DDL to CSV format
        csv_buffer = io.StringIO()
        csv_writer = csv.writer(csv_buffer)
        csv_writer.writerow([view_name, ddl])  # Write view name and its DDL
        
        # Generate the filename with 'V_' prefix
        filename = f"V_{view_name}_DDL.csv"
        
        # Upload to S3 using boto3
        s3_key = f"{S3_PATH}{filename}"
        logger.info(f"Uploading DDL CSV to S3 bucket: {S3_BUCKET}, path: {s3_key}")
        s3 = boto3.client('s3')
        s3.put_object(Bucket=S3_BUCKET, Key=s3_key, Body=csv_buffer.getvalue())
        
        logger.info(f"File successfully uploaded to S3: s3://{S3_BUCKET}/{s3_key}")
    except Exception as e:
        logger.error(f"Error uploading DDL CSV for view {view_name} to S3: {e}")
        raise

def main():
    try:
        logger.info("Starting the process of fetching DDL from Redshift views and uploading to S3.")
        
        # Get Redshift connection
        conn = get_redshift_connection()
        
        # Get list of all views in the schema
        views = get_views_in_schema(conn, REDSHIFT_SCHEMA)
        
        # Loop through all views and process each one
        for view in views:
            logger.info(f"Processing view: {view}")
            
            # Fetch DDL for the Redshift view
            ddl = fetch_view_ddl(conn, view, REDSHIFT_SCHEMA)
            
            if ddl:
                # Upload the DDL to S3
                upload_to_s3(ddl, view)
            else:
                logger.warning(f"Skipping view {view} as no DDL was found.")
    
    except Exception as e:
        logger.error(f"Script failed due to: {e}")
    finally:
        if 'conn' in locals() and conn:
            conn.close()
            logger.info("Redshift connection closed.")

if __name__ == '__main__':
    main()
