
msg:
"001003 (42000): 01bc01d1-010d-27b4-003c-3a073f139b8a: SQL compilation error:\nsyntax error line 1 at position 174 unexpected '_type'."
query:
'CREATE OR REPLACE VIEW "CERBA"."code_validation_vw" AS  SELECT DISTINCT sid."STUDY"_id, v.vendor_name, \'sm_code\'::text AS value_type, sp."SPECIMEN"_type, sp.vendor_"SPECIMEN"_type AS orig_value, \n        CASE\n            WHEN sp.vendor_id = 3 AND sp.vendor_"SPECIMEN"_type::text ~~ \'SM%\'::text THEN "substring"(sp.vendor_"SPECIMEN"_type::text, 1, 4)\n            WHEN sp.vendor_id = 3 AND sp.vendor_"SPECIMEN"_type::text ~~ \'HM%\'::text THEN "substring"(sp.vendor_"SPECIMEN"_type::text, 1, 4)\n            WHEN sp.vendor_id = 3 AND sp."SPECIMEN"_class::text ~~ \'TBO%\'::text THEN "substring"(sp."SPECIMEN"_class::text, 1, 6)\n            WHEN sp.vendor_id = 21 THEN sp."SPECIMEN"_class::text\n            ELSE NULL::text\n        END::character varying AS value, c.report_value_type, \n        CASE\n            WHEN sp.vendor_id = 3 THEN c.report_value\n            WHEN sp.vendor_id = 21 THEN initcap(sp."SPECIMEN"_type::text)::character varying\n            ELSE NULL::character varying\n        END AS report_va...
raw_msg:
"SQL compilation error:\nsyntax error line 1 at position 174 unexpected '_type'."
sfqid:
'01bc01d1-010d-27b4-003c-3a073f139b8a'
sqlstate:
'42000'




import psycopg2
import snowflake.connector
import logging
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import serialization

# -------------------------------------------
# Redshift connection configuration
# -------------------------------------------
REDSHIFT_HOST = 'medai-orbit.cqibeghf1hd5.us-east-1.redshift.amazonaws.com'
REDSHIFT_PORT = '5439'  # Default Redshift port
REDSHIFT_USER = 'orbit_sa_read_only'
REDSHIFT_PASSWORD = '419iKpjj-zPX'
REDSHIFT_DB = 'orbit'

# -------------------------------------------
# Snowflake connection configuration
# -------------------------------------------
sf_user = 'SA-JRDUS-ORBIT-ENG-D'
sf_account = 'jrd-bt-useast.privatelink'
sf_warehouse = 'WH_ORBIT_DEV_ETL'
sf_database = 'ORBIT_DEV'
sf_schema = 'CERBA'
sf_role = 'ITS_APP_DEV_RDDW_ORBIT_DEVELOPERS'

# Path to your private key (.p8 or .pem)
PRIVATE_KEY_PATH = 'sa-jrdus-orbit-eng-d_rsa_key.p8'  # ðŸ” UPDATE THIS
PRIVATE_KEY_PASSPHRASE = b'0rB1THkjSu9'  # If applicable, replace with your passphrase

# -------------------------------------------
# Set up logging
# -------------------------------------------
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,  # Change to DEBUG for more detailed logs
    filename='view_migration.log',
    filemode='w'
)

# -------------------------------------------
# Load private key for Snowflake
# -------------------------------------------
def get_private_key():
    with open(PRIVATE_KEY_PATH, "rb") as key_file:
        private_key = serialization.load_pem_private_key(
            key_file.read(),
            password=PRIVATE_KEY_PASSPHRASE,
            backend=default_backend()
        )
    return private_key.private_bytes(
        encoding=serialization.Encoding.DER,
        format=serialization.PrivateFormat.PKCS8,
        encryption_algorithm=serialization.NoEncryption()
    )

# -------------------------------------------
# Function to connect to Redshift
# -------------------------------------------
def connect_to_redshift():
    try:
        connection = psycopg2.connect(
            dbname=REDSHIFT_DB,
            user=REDSHIFT_USER,
            password=REDSHIFT_PASSWORD,
            host=REDSHIFT_HOST,
            port=REDSHIFT_PORT
        )
        logging.info("Connected to Redshift successfully.")
        return connection
    except Exception as e:
        logging.error(f"Error connecting to Redshift: {e}")
        raise

# -------------------------------------------
# Function to connect to Snowflake with Role and Private Key
# -------------------------------------------
def connect_to_snowflake():
    try:
        private_key = get_private_key()

        connection = snowflake.connector.connect(
            user=sf_user,
            account=sf_account,
            private_key=private_key,
            warehouse=sf_warehouse,
            database=sf_database,
            schema=sf_schema,
            role=sf_role  # Include role
        )
        logging.info("Connected to Snowflake successfully.")
        return connection
    except Exception as e:
        logging.error(f"Error connecting to Snowflake: {e}")
        raise

# -------------------------------------------
# Function to fetch the correct case of tables and views in Snowflake
# -------------------------------------------
def fetch_snowflake_objects():
    connection = connect_to_snowflake()
    cursor = connection.cursor()

    # Fetch tables
    cursor.execute(f"SHOW TABLES IN SCHEMA {sf_database}.{sf_schema}")
    tables = {row[1].lower(): row[1] for row in cursor.fetchall()}  # lowercase -> original case

    # Fetch views
    cursor.execute(f"SHOW VIEWS IN SCHEMA {sf_database}.{sf_schema}")
    views = {row[1].lower(): row[1] for row in cursor.fetchall()}  # lowercase -> original case

    # Combine tables and views into one mapping
    object_mapping = {**tables, **views}
    
    connection.close()
    logging.info(f"Fetched {len(object_mapping)} objects (tables and views) from Snowflake.")
    return object_mapping

# -------------------------------------------
# Function to fetch the view definitions from Redshift
# -------------------------------------------
def fetch_redshift_views(view_name=None):
    connection = connect_to_redshift()
    cursor = connection.cursor()

    # Adjust query if a specific view name is provided
    if view_name:
        cursor.execute(f"""
            SELECT 
                v.table_name,
                pg_get_viewdef(quote_ident(v.table_schema) || '.' || quote_ident(v.table_name), true) AS view_definition
            FROM information_schema.views v
            WHERE v.table_schema = 'orbit_64407564mmy3009' AND v.table_name = '{view_name}'
        """)
    else:
        cursor.execute("""
            SELECT 
                v.table_name,
                pg_get_viewdef(quote_ident(v.table_schema) || '.' || quote_ident(v.table_name), true) AS view_definition
            FROM information_schema.views v
            WHERE v.table_schema = 'orbit_64407564mmy3009'
        """)

    views = cursor.fetchall()
    connection.close()

    logging.info(f"Fetched {len(views)} views from Redshift.")
    return views

# -------------------------------------------
# Function to map Redshift table names to Snowflake table names in view definitions
# -------------------------------------------
# -------------------------------------------
# Function to map Redshift table names to Snowflake table names in view definitions
# -------------------------------------------
def map_table_names_in_view(view_definition, object_mapping):
    if view_definition is None:
        logging.warning("View definition is None, cannot map table names.")
        return None  # Return None to indicate the view cannot be processed

    # Loop through the object mapping for tables and views
    for redshift_name, snowflake_name in object_mapping.items():
        # Ensure that both the table and column names in the view are properly quoted in Snowflake
        quoted_redshift_name = f'"{redshift_name}"'  # Ensure Redshift name is quoted
        quoted_snowflake_name = f'"{snowflake_name}"'  # Ensure Snowflake name is quoted
        
        # Replace occurrences of unquoted Redshift names with Snowflake equivalents
        view_definition = view_definition.replace(f'{redshift_name}', quoted_snowflake_name)
        
        # Also replace quoted instances if any (e.g., "table_name" -> "table_name" in Snowflake)
        view_definition = view_definition.replace(f'"{redshift_name}"', quoted_snowflake_name)
    
    # Ensure all columns that include underscores are correctly quoted in Snowflake
    view_definition = view_definition.replace('"_type"', '"_type"')  # Fix underscores in column names
    view_definition = view_definition.replace('"SPECIMEN"_type', '"SPECIMEN"_type')  # Fix this specific case
    
    return view_definition

    if view_definition is None:
        logging.warning("View definition is None, cannot map table names.")
        return None  # Return None to indicate the view cannot be processed

    for redshift_name, snowflake_name in object_mapping.items():
        # Ensure that both the table and column names in the view are properly quoted in Snowflake
        quoted_redshift_name = f'"{redshift_name}"'  # Ensure Redshift name is quoted
        quoted_snowflake_name = f'"{snowflake_name}"'  # Ensure Snowflake name is quoted
        
        # Replace occurrences of unquoted Redshift names with Snowflake equivalents
        view_definition = view_definition.replace(f'{redshift_name}', quoted_snowflake_name)
        
        # Also replace quoted instances if any (e.g., "table_name" -> "table_name" in Snowflake)
        view_definition = view_definition.replace(f'"{redshift_name}"', quoted_snowflake_name)
    
    return view_definition

# -------------------------------------------
# Function to create views in Snowflake
# -------------------------------------------
def create_view_in_snowflake(view_name, view_definition):
    if view_definition is None:
        logging.error(f"Cannot create view {view_name}: view definition is None.")
        return

    connection = connect_to_snowflake()
    cursor = connection.cursor()

    try:
        # Ensure the view name is quoted correctly with the schema and view name
        quoted_view_name = f'"{sf_schema}"."{view_name}"'

        # Create the view in Snowflake
        create_view_query = f'CREATE OR REPLACE VIEW {quoted_view_name} AS {view_definition}'
        cursor.execute(create_view_query)
        connection.commit()
        logging.info(f"Created view {view_name} in Snowflake.")
    except Exception as e:
        logging.error(f"Error creating view {view_name} in Snowflake: {e}")
        connection.rollback()
    finally:
        connection.close()

# -------------------------------------------
# Function to migrate views from Redshift to Snowflake
# -------------------------------------------
def migrate_views_to_snowflake(view_name=None):
    try:
        # Fetch views from Redshift (either all or one specific view)
        views = fetch_redshift_views(view_name)

        # Fetch tables and views from Snowflake
        object_mapping = fetch_snowflake_objects()

        # Migrate each view
        for view in views:
            view_name, view_definition = view
            logging.info(f"Migrating view {view_name}...")

            # Map Redshift object names to Snowflake object names
            snowflake_view_definition = map_table_names_in_view(view_definition, object_mapping)
            if snowflake_view_definition is None:
                logging.warning(f"Skipping view {view_name} due to missing definition.")
                continue

            # Adjust for Snowflake-specific syntax (e.g., date functions)
            snowflake_view_definition = snowflake_view_definition.replace(
                "(('now'::text)::date - ('1 year'::interval * (\"SECOND\".age)::double precision))",
                "DATEADD(YEAR, - \"SECOND\".age, CURRENT_DATE)"
            )

            # Create the view in Snowflake
            create_view_in_snowflake(view_name, snowflake_view_definition)

        logging.info("View migration from Redshift to Snowflake completed successfully.")
    except Exception as e:
        logging.error(f"Error during view migration: {e}")
        raise

# -------------------------------------------
# Main execution
# -------------------------------------------
if __name__ == "__main__":
    try:
        # Migrate only the specific view, e.g., 'code_validation_vw'
        migrate_views_to_snowflake(view_name="code_validation_vw")
    except Exception as e:
        logging.critical(f"Critical error during view migration: {e}")
