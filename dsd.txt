import psycopg2
import logging
import boto3
import io

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(), logging.FileHandler('migration.log')]
)

# AWS S3 credentials and bucket name
AWS_BUCKET_NAME = '-'
AWS_REGION = 'us-east-1'
AWS_ACCESS_KEY = '#'
AWS_SECRET_KEY = '-'

# Redshift connection details
REDSHIFT_HOST = '-'
REDSHIFT_PORT = '5439'  # Default Redshift port
REDSHIFT_USER = '-'
REDSHIFT_PASSWORD = '-'
REDSHIFT_DB = 'orbit'

# S3 client setup
s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY, region_name=AWS_REGION)

# Function to connect to Redshift
def connect_redshift():
    try:
        conn = psycopg2.connect(
            dbname=REDSHIFT_DB,
            user=REDSHIFT_USER,
            password=REDSHIFT_PASSWORD,
            host=REDSHIFT_HOST,
            port=REDSHIFT_PORT
        )
        logging.info("Successfully connected to Redshift.")
        return conn
    except Exception as e:
        logging.error(f"Error connecting to Redshift: {e}")
        raise

# Function to get view names (only views, not tables)
def get_view_names():
    try:
        conn = connect_redshift()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT table_name 
            FROM information_schema.views 
            WHERE table_schema = 'orbit_64407564mmy3009';
        """)
        views = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        logging.info(f"Retrieved view names: {views}")
        return views
    except Exception as e:
        logging.error(f"Error fetching view names: {e}")
        raise

# Function to get the full DDL (CREATE VIEW) of a view
def get_view_ddl(view_name):
    try:
        conn = connect_redshift()
        cursor = conn.cursor()
        cursor.execute(f"""
            SELECT pg_catalog.pg_get_viewdef('{view_name}', true);
        """)
        ddl = cursor.fetchone()[0]  # Fetch the full CREATE VIEW DDL
        cursor.close()
        conn.close()
        logging.info(f"Retrieved full DDL for view {view_name}.")
        return ddl
    except Exception as e:
        logging.error(f"Error retrieving DDL for view {view_name}: {e}")
        raise

# Function to migrate view DDL to S3
def migrate_view_ddl_to_s3(view_name):
    try:
        ddl = get_view_ddl(view_name)
        
        # Upload the full DDL to S3 as a text file
        s3_key = f"snowflake_poc/{view_name}.sql"
        s3_client.put_object(Bucket=AWS_BUCKET_NAME, Key=s3_key, Body=ddl)
        logging.info(f"Uploaded full DDL for view {view_name} to S3.")
    
    except Exception as e:
        logging.error(f"Error migrating DDL for view {view_name}: {e}")
        raise

# Function to migrate DDL for all views
def migrate_ddl_to_s3():
    try:
        # Migrate views and their full DDL
        views = get_view_names()
        for view in views:
            logging.info(f"Starting export for view DDL: {view}")
            
            # Migrate full DDL for the view
            migrate_view_ddl_to_s3(view)
            logging.info(f"Successfully exported DDL for view: {view}")
    
    except Exception as e:
        logging.error(f"Migration failed: {e}")
        raise

# Execute the migration
if __name__ == "__main__":
    logging.info("Migration process started.")
    try:
        migrate_ddl_to_s3()
        logging.info("All view DDL migrated successfully.")
    except Exception as e:
        logging.error(f"Migration process failed: {e}")
